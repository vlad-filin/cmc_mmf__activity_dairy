{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tacotron: towards end-to-end speech synthesis\n",
    "date 6.04.2017, paper on [arxiv](https://arxiv.org/pdf/1703.10135.pdf)\n",
    "__________________\n",
    "## Model overview\n",
    "______________\n",
    "Since models like wavenets requires external models for prediction of linguistic features (grapheme to phoneme, fundamental frequency, phoneme duration) which are trained independently (and joint training reported to be complicated) these models are not end-to-end. In this model authors used as input - characters, and output - spectrogram, which is then converted to raw wavenet using Griffin-Lim algorithm.\n",
    "### Model architecture\n",
    "Model used several blocks (will be discussed later)\n",
    "* CBHG module;\n",
    "* Pre-net (2 stack linear layer with dropout);\n",
    "* Attention RNN;\n",
    "* Decoder RNN.\n",
    "* Post-net\n",
    "\n",
    "Overall architecture is shown below:\n",
    "![model](images/model_mod.jpg)\n",
    "\n",
    "#### CBHG module\n",
    "<img src=\"images/cbhg_mod.jpg\" width=\"300\" height=\"314\">\n",
    "\n",
    "\n",
    "In CBHG module all convolution is done on time axis (e.g. using PyTorch notation if you have batch size B in each batch size T characters with embedding size E after pre-net then input tensor to CBHG module has size (B, T, E) and in CBHG convolutions is done on dimension T, it could be done via swapping E and T dimension and using nn.conv1d)\n",
    "\n",
    "All convolutions must preserve the time dimension (working dimension) and since stride=1, must use padding = kernel_size // 2.\n",
    "\n",
    "After convolution is done, working dimension (dimension for matrix multiplication in linear layers) becomes E.\n",
    "Highway network consists of 4 highway blocks. Highway block is shown below (T = linear layer with sigmoid activation, C equals to 1 - T, and H is linear layer with relu activation).\n",
    "\n",
    "<img src=\"images/highway.png\" width=\"300\" height=\"314\">\n",
    "\n",
    "After applying highway network bidirectional GRU is applied.\n",
    "#### Attention\n",
    "\n",
    "It is not properly covered in article, so I looked into.code](https://github.com/r9y9/tacotron_pytorch/blob/master/tacotron_pytorch/attention.py).\n",
    "At each decoder timestep query $q_t$ is calculated as output of decoder prenet. Attention is calculated between query $q_t \\in R^{Batch \\times 1 \\times dim}$ and encoder outputs $O \\in R^{Batch \\times T \\times dim}$ as following and used for calculation context vector as following:\n",
    "\n",
    "$$\n",
    "attention_t = softmax(Linear(tanh(Linear(q, W) + O)), v) \\in R^{Batch \\times 1 \\times T},\n",
    "$$\n",
    "$$\n",
    "context \\ vector_t = attention_t @ O \\in R^{Batch \\times 1 \\times dim},\n",
    "$$\n",
    "where @ is batch matrix-matrix multiplication (simple matrix matrix multiplication for every element in batch, check torch.bmm), $ Linear(x, v):  R^{Batch \\times T \\times dim} -> R^{Batch \\times 1 \\times T}$ - linear transformation (dim -> 1) with unsqueezing dimension and $ Linear(x, w):  R^{Batch \\times 1 \\times dim} -> R^{Batch \\times 1 \\times dim}$ - linear transformation.\n",
    "\n",
    "#### Attention RNN\n",
    "Attention RNN does pass input through RNN cell and uses its output to calculate context vector using attention.\n",
    "At each timestep context vector and the RNN cell output are concatenated to form the input to the decoder RNN. \n",
    "#### Decoder RNN\n",
    "\n",
    "At each timestep decoder rnn outputs several non-overlapping  mel spectrograms (vectors). Predicting $r$ frames at once divides the total number of decoder steps by $r$, which reduces model size, training time and inference time.\n",
    "Authors also reported this trick to substantially increase convergence speed, as measured by a much faster (and more stable) alignment learned from attention.\n",
    "\n",
    "#### Post-Net\n",
    "\n",
    "Since model uses Griffin-Lim algorithm for synthesis raw waveforms, mel spectrograms must be converted to linear spectrograms. Post-net takes all generated mel spectrograms and passes it through another  CBHG module to predict linear spectrograms.\n",
    "\n",
    "#### Training\n",
    "\n",
    "During training groundtruth mel-spectrogram of previous frame used as input to decoder (for the first frame 0 spectrogram is used). L1 loss was used for both linear and mel spectrograms. Authors also report that learning model to predict zero spectrogram after stop token is essential for model to learn where to stop generating.\n",
    "\n",
    "#### Testing \n",
    "\n",
    "During testing previous generated mel spectrogram used as input to decoder (for the first frame 0 spectrogram is used). After sequential generating of all mel-spectrograms, they are used as input to post-net to generate linear spectrograms, which are used as input to Griffin-Lim algorithm to generate raw waveforms.\n",
    "This model doesn't have time drawback of the wavenet even using sequential generating since spectrograms are highly compressed in time domain comparing to raw waveforms.\n",
    "\n",
    "#### Experiments\n",
    "\n",
    "Internal North American English dataset, which contains about 24.6 hours of speech data spoken by a professional female speaker, was used for training. Mean opinion score was used for evaluating. Model performed better than parametric model using LSTM, but worse than concatenating approach. There were no compliance in the article with wavenet model, but wavenet model was reported (in wavenet paper) to outperform concatenation approach.\n",
    "\n",
    "#### Pros and cons\n",
    "\n",
    "The main pros of the model is being really end-to-end (excluding Griffin-Lim), models doesn't use complicated linguistic features as wavenet model. Original Wavenet model used external (unpublished!) methods to predict linguistic features, and Deep Voice version of wavenet used a lot of DL models for predicting these features, however all these models are trained separately and tuning model of other language is not trivial and requires much time and expertise. The sequential generation is not a drawback of Tacotron (unlike wavenet) due high compression of spectrograms in time domain comparing to raw waveforms. However, the quality of generated samples are significantly lower comparing to wavenet generation (Listened to several samples with same text). Tacotron samples tends to have metallic pitches and sometimes skips words in long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
